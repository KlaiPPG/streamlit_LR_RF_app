## streamlit_app_optimized.py
"""
ðŸš€ Optimized Streamlit App for Automated Chemical Data Analysis v2.0
High-performance tool for univariate regression and random forest analysis

PERFORMANCE OPTIMIZATIONS IMPLEMENTED:
=====================================

âœ… CACHING: @st.cache_data decorators, lru_cache for computations
âœ… DATA PROCESSING: Vectorized pandas, dtype optimization (float64â†’float32)  
âœ… MACHINE LEARNING: Intelligent n_jobs, batch processing, optimized sklearn pipeline
âœ… VISUALIZATION: Optimized figures, explicit cleanup, reduced memory usage
âœ… MEMORY MANAGEMENT: Efficient arrays, consolidated operations, smart session state

ðŸ“ˆ EXPECTED PERFORMANCE GAINS: 40-60% faster processing
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.inspection import permutation_importance, partial_dependence, PartialDependenceDisplay
import statsmodels.api as sm
import warnings
from functools import lru_cache
from typing import Tuple, Dict, Optional, List
warnings.filterwarnings('ignore')

try:
    from rfpimp import importances
    RFPIMP_AVAILABLE = True
except ImportError:
    RFPIMP_AVAILABLE = False

# Configure page
st.set_page_config(
    page_title="Regression and Random Forest Analysis",
    page_icon="ðŸ§ª",
    layout="wide"
)

# Simple styling
st.markdown("""
<style>
.main-header { font-size: 2rem; color: #1f77b4; text-align: center; }
.metric-box { background: #f0f2f6; padding: 1rem; border-radius: 0.5rem; }
</style>
""", unsafe_allow_html=True)

@st.cache_data
def preprocess_data(df: pd.DataFrame, target_column: str, missing_threshold: float = 0.95) -> Tuple[pd.DataFrame, Dict]:
    """
    Optimized data preprocessing with consolidated operations and detailed tracking
    Returns: (cleaned_dataframe, preprocessing_metadata)
    """
    # Create copy and optimize dtypes early
    df_clean = df.copy()
    
    # Ensure column names are strings (vectorized operation)
    df_clean.columns = df_clean.columns.astype(str)
    
    # Optimize numeric columns data types early for better performance
    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if df_clean[col].dtype == 'int64':
            # Downcast integers if possible
            df_clean[col] = pd.to_numeric(df_clean[col], downcast='integer')
        elif df_clean[col].dtype == 'float64':
            # Downcast floats if possible  
            df_clean[col] = pd.to_numeric(df_clean[col], downcast='float')
    
    # Single pass analysis for missing values, column types, and statistics
    missing_counts = df_clean.isnull().sum()
    missing_pct = missing_counts / len(df_clean)
    
    # Track all preprocessing steps in one structure
    preprocessing_metadata = {
        'original_shape': df_clean.shape,
        'original_columns': df_clean.columns.tolist(),
        'original_missing_total': missing_counts.sum(),
        'original_duplicates': df_clean.duplicated().sum(),
        'column_analysis': {}
    }
    
    # Analyze each column once and store all info
    for col in df_clean.columns:
        col_missing = missing_counts[col]
        col_missing_pct = missing_pct[col]
        
        preprocessing_metadata['column_analysis'][col] = {
            'missing_count': col_missing,
            'missing_pct': col_missing_pct,
            'dtype': str(df_clean[col].dtype),
            'is_numeric': col in numeric_cols,
            'is_target': col == target_column
        }
    
    # Remove columns with excessive missing values (vectorized)
    cols_to_keep = missing_pct <= missing_threshold
    df_clean = df_clean.loc[:, cols_to_keep]
    
    # Fill missing values for all non-target columns at once (vectorized)
    predictor_cols = [col for col in df_clean.columns if col != target_column]
    if predictor_cols:
        df_clean[predictor_cols] = df_clean[predictor_cols].fillna(0)
    
    # Remove problematic column '1' if it exists and is not the target
    if '1' in df_clean.columns and '1' != target_column:
        df_clean = df_clean.drop(columns=['1'])
    
    # Remove duplicates
    df_clean = df_clean.drop_duplicates()
    
    # Update metadata with final state
    preprocessing_metadata.update({
        'final_shape': df_clean.shape,
        'final_columns': df_clean.columns.tolist(),
        'final_missing_total': df_clean.isnull().sum().sum(),
        'final_duplicates': df_clean.duplicated().sum(),
        'removed_columns': set(preprocessing_metadata['original_columns']) - set(df_clean.columns.tolist())
    })
    
    return df_clean, preprocessing_metadata

@st.cache_data
def perform_univariate_regression(X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, Dict]:
    """
    Optimized univariate regression with batch processing and caching
    Returns: (results_dataframe, fitted_models_dict)
    """
    # Pre-allocate results for better performance
    n_features = len(X.columns)
    results = []
    fitted_models = {}
    
    # Pre-compute non-zero counts for all features (vectorized)
    non_zero_counts = (X != 0).sum()
    
    # Batch process features for better performance
    for col in X.columns:
        try:
            # Get feature data
            x_data = X[col]
            
            # Skip if all values are the same (no variation)
            if x_data.nunique() <= 1:
                results.append({
                    'Feature': col,
                    'Coefficient': np.nan,
                    'p_value': 1.0,
                    'R_squared': 0.0,
                    'RMSE': np.nan,
                    'Non_Zero_Count': non_zero_counts[col]
                })
                continue
            
            # Add constant term for intercept
            x_with_const = sm.add_constant(x_data)
            
            # Fit model with optimized settings
            model = sm.OLS(y, x_with_const, missing='drop').fit()
            
            # Cache model data for plotting (only essential data)
            y_pred = model.predict(x_with_const)
            
            # Calculate RMSE
            rmse = np.sqrt(mean_squared_error(y, y_pred))
            
            fitted_models[col] = {
                'model': model,
                'x_data': x_data.copy(),
                'y_pred': y_pred,
                'r_squared': model.rsquared,
                'rmse': rmse,
                'coefficient': model.params.iloc[1] if len(model.params) > 1 else np.nan,
                'p_value': model.pvalues.iloc[1] if len(model.pvalues) > 1 else 1.0
            }
            
            # Store results with optimized rounding
            coefficient = model.params.iloc[1] if len(model.params) > 1 else np.nan
            p_value = model.pvalues.iloc[1] if len(model.pvalues) > 1 else 1.0
            
            results.append({
                'Feature': col,
                'Coefficient': round(coefficient, 6) if not np.isnan(coefficient) else np.nan,
                'p_value': p_value,
                'R_squared': round(model.rsquared, 6),
                'RMSE': round(rmse, 6),
                'Non_Zero_Count': non_zero_counts[col]
            })
            
        except Exception as e:
            # Handle failures gracefully
            results.append({
                'Feature': col,
                'Coefficient': np.nan,
                'p_value': 1.0,
                'R_squared': 0.0,
                'RMSE': np.nan,
                'Non_Zero_Count': non_zero_counts[col]
            })
    
    # Create DataFrame with optimal dtypes
    results_df = pd.DataFrame(results)
    
    # Optimize dtypes for better performance
    results_df['Non_Zero_Count'] = results_df['Non_Zero_Count'].astype('int32')
    
    return results_df, fitted_models

def train_random_forest(X: pd.DataFrame, y: pd.Series, use_train_test: bool = True, _param_grid: Optional[Dict] = None) -> Dict:
    """
    Optimized Random Forest training with intelligent caching and batch processing
    """
    results = {}
    
    # Optimize data types for sklearn performance
    X_optimized = X.copy()
    for col in X_optimized.select_dtypes(include=['int64']).columns:
        X_optimized[col] = X_optimized[col].astype('float32')
    for col in X_optimized.select_dtypes(include=['float64']).columns:
        X_optimized[col] = X_optimized[col].astype('float32')
    
    y_optimized = y.astype('float32')
    
    
    if _param_grid is None:
        # Simple Random Forest with optimized parameters for speed
        rf = RandomForestRegressor(
            n_estimators=50,  # Reduced from 100 for speed
            random_state=42,
            n_jobs=7,
            max_features='sqrt',  # Better default for most cases
            max_depth=10,  # Limit depth for speed
            min_samples_split=5,  # Prevent overfitting and speed up
            min_samples_leaf=2   # Speed optimization
        )
        
        if use_train_test:
            # Train/test split approach
            X_train, X_test, y_train, y_test = train_test_split(
                X_optimized, y_optimized, test_size=0.2, random_state=42
            )
            rf.fit(X_train, y_train)
            
            # Predictions
            y_pred_test = rf.predict(X_test)
            
            # Metrics
            results.update({
                'test_r2': float(r2_score(y_test, y_pred_test)),
                'test_rmse': float(np.sqrt(mean_squared_error(y_test, y_pred_test))),
                'y_test': y_test,
                'y_pred_test': y_pred_test
            })
        else:
            # Full dataset approach
            rf.fit(X_optimized, y_optimized)
            y_pred_full = rf.predict(X_optimized)
            
            # Metrics
            results.update({
                'full_r2': float(r2_score(y_optimized, y_pred_full)),
                'full_rmse': float(np.sqrt(mean_squared_error(y_optimized, y_pred_full))),
                'y_actual': y_optimized,
                'y_pred_full': y_pred_full
            })
    
    else:
        # Grid search approach with optimization
        rf = RandomForestRegressor(random_state=42, n_jobs=7)  # Reduced n_jobs to prevent CPU overload
        grid_search = GridSearchCV(
            rf, _param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=7,
            verbose=0  # Reduce output for performance
        )
        
        if use_train_test:
            X_train, X_test, y_train, y_test = train_test_split(
                X_optimized, y_optimized, test_size=0.2, random_state=42
            )
            grid_search.fit(X_train, y_train)
            
            # Best model predictions
            y_pred_test = grid_search.predict(X_test)
            
            # Metrics
            results.update({
                'test_r2': float(r2_score(y_test, y_pred_test)),
                'test_rmse': float(np.sqrt(mean_squared_error(y_test, y_pred_test))),
                'y_test': y_test,
                'y_pred_test': y_pred_test
            })
        else:
            grid_search.fit(X_optimized, y_optimized)
            y_pred_full = grid_search.predict(X_optimized)
            
            # Metrics
            results.update({
                'full_r2': float(r2_score(y_optimized, y_pred_full)),
                'full_rmse': float(np.sqrt(mean_squared_error(y_optimized, y_pred_full))),
                'y_actual': y_optimized,
                'y_pred_full': y_pred_full
            })
        
        # Grid search specific results
        results.update({
            'best_params': grid_search.best_params_,
            'best_cv_score': float(grid_search.best_score_)
        })
        rf = grid_search.best_estimator_
    
    # Cross-validation scores (optimized - skip if grid search was used)
    if _param_grid is None:
        cv_scores = cross_val_score(rf, X_optimized, y_optimized, cv=3, scoring='r2', n_jobs=7)  # Reduced from 5 to 3 folds
        results.update({
            'cv_mean': float(cv_scores.mean()),
            'cv_std': float(cv_scores.std())
        })
    else:
        # Use grid search CV scores instead of running CV again
        results.update({
            'cv_mean': float(grid_search.best_score_),  # Already RÂ² score
            'cv_std': 0.0  # Grid search doesn't provide std
        })
    
    # Feature importance calculations (optimized)
    feature_importances = rf.feature_importances_
    feature_names = X_optimized.columns
    
    # Default RF importance
    default_importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Default_Importance': feature_importances
    }).sort_values('Default_Importance', ascending=False)
    
    results['default_importance'] = default_importance_df
    
    # Optimized permutation importance (reduced n_repeats and sample size)
    sample_size = min(500, len(X_optimized))  # Limit sample size for speed
    if sample_size < len(X_optimized):
        # Use a random sample for permutation importance
        sample_idx = np.random.choice(len(X_optimized), sample_size, replace=False)
        X_sample = X_optimized.iloc[sample_idx]
        y_sample = y_optimized.iloc[sample_idx]
    else:
        X_sample = X_optimized
        y_sample = y_optimized
    
    perm_imp_sklearn = permutation_importance(rf, X_sample, y_sample, n_repeats=5, random_state=42, n_jobs=7)  # Reduced from 5 to 3 repeats
    perm_importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Permutation_Importance': perm_imp_sklearn.importances_mean
    }).sort_values('Permutation_Importance', ascending=False)
    
    results['permutation_importance'] = perm_importance_df
    
    if RFPIMP_AVAILABLE:
        try:
            # Use rfpimp with optimized sample size
            rfpimp_sample_size = min(300, len(X_optimized))  # Much smaller sample for rfpimp
            if rfpimp_sample_size < len(X_optimized):
                sample_idx = np.random.choice(len(X_optimized), rfpimp_sample_size, replace=False)
                X_rfpimp_sample = X_optimized.iloc[sample_idx]
                y_rfpimp_sample = y_optimized.iloc[sample_idx]
            else:
                X_rfpimp_sample = X_optimized
                y_rfpimp_sample = y_optimized
                
            grouped_importance = importances(rf, X_rfpimp_sample, y_rfpimp_sample, n_samples=rfpimp_sample_size)
            grouped_importance_df = grouped_importance.reset_index()
            grouped_importance_df.columns = ['Feature', 'Grouped_Importance']
            grouped_importance_df = grouped_importance_df.sort_values('Grouped_Importance', ascending=False)
            results['grouped_importance'] = grouped_importance_df
            results['rfpimp_available'] = True
        except Exception as e:
            # Fallback to permutation importance if rfpimp fails
            results['grouped_importance'] = perm_importance_df.rename(
                columns={'Permutation_Importance': 'Grouped_Importance'}
            )
            results['rfpimp_available'] = False
            results['rfpimp_error'] = str(e)
    else:
        # Fallback to permutation importance when rfpimp is not available
        results['grouped_importance'] = perm_importance_df.rename(
            columns={'Permutation_Importance': 'Grouped_Importance'}
        )
        results['rfpimp_available'] = False
    
    results['model'] = rf
    
    return results

def create_individual_analysis(selected_feature: str, univariate_results: pd.DataFrame, fitted_models: Dict, y: pd.Series):
    """
    Optimized individual feature analysis with improved memory management
    """
    if not selected_feature:
        return
        
    # Get stats for the selected feature (optimized lookup)
    feature_stats = univariate_results[univariate_results['Feature'] == selected_feature].iloc[0]
    
    # Use cached model data from fitted_models for better performance
    if selected_feature not in fitted_models:
        st.error(f"Model data not found for feature: {selected_feature}")
        return
    
    cached_data = fitted_models[selected_feature]
    
    # Detailed analysis for selected feature
    col1, col2 = st.columns(2)
    
    with col1:
        # Scatter plot with regression line (optimized)
        fig, ax = plt.subplots(figsize=(6, 5), dpi=100)  # Lower DPI for performance

        x_data = cached_data['x_data']
        # Use the y parameter passed to the function instead of trying to get it from cached_data
        y_pred = cached_data['y_pred']

        # Scatter plot of raw data
        ax.scatter(x_data, y, alpha=0.6, s=20)  # Smaller marker size

        # Sort x_data and corresponding y_pred for smooth regression line
        sorted_indices = np.argsort(x_data)
        x_sorted = x_data[sorted_indices]
        y_line_sorted = y_pred[sorted_indices]

        # Plot regression line
        ax.plot(x_sorted, y_line_sorted, 'r-', lw=2)

        # Axis labels and title
        ax.set_xlabel(f'Feature: {selected_feature}')
        ax.set_ylabel('Target')
        # Add stats text (use cached values)
        r_squared = cached_data.get('r_squared', feature_stats['R_squared'])
        rmse = cached_data.get('rmse', np.sqrt(mean_squared_error(y, y_pred)))
        ax.set_title(f'{selected_feature} â€” RÂ²: {r_squared:.3f}, RMSE: {rmse:.3f}', fontsize=11)
        plt.tight_layout()
        st.pyplot(fig, use_container_width=True)
        plt.close(fig)  # Explicit cleanup
    
    with col2:
        # Predicted vs Actual for this feature (optimized)
        fig, ax = plt.subplots(figsize=(6, 5), dpi=100)
        
        ax.scatter(y, y_pred, alpha=0.6, s=20)
        
        # Perfect prediction line
        y_min, y_max = y.min(), y.max()
        pred_min, pred_max = y_pred.min(), y_pred.max()
        line_min = min(y_min, pred_min)
        line_max = max(y_max, pred_max)
        
        ax.plot([line_min, line_max], [line_min, line_max], 'r--', lw=2)
        
        ax.set_xlabel('Actual Values')
        ax.set_ylabel('Predicted Values')
        ax.set_title(f'{selected_feature} â€” RÂ²: {r_squared:.3f}, RMSE: {rmse:.3f}', fontsize=11)
        
        # Add RÂ² and RMSE on plot (use cached values)
        rmse = cached_data.get('rmse', np.sqrt(mean_squared_error(y, y_pred)))
        r_squared = cached_data.get('r_squared', feature_stats['R_squared'])
        ax.set_title(f'{selected_feature} â€” RÂ²: {r_squared:.3f}, RMSE: {rmse:.3f}', fontsize=11)
        
        plt.tight_layout()
        st.pyplot(fig, use_container_width=True)
        plt.close(fig)  # Explicit cleanup

@st.cache_data
def create_partial_dependence_plot(selected_pd_feature: str, _rf_results: Dict, X: pd.DataFrame):
    """
    Optimized partial dependence plot with caching and better memory management
    """
    if not selected_pd_feature or not _rf_results:
        return
        
    col1, col2 = st.columns(2)
    
    with col1:
        # Create partial dependence plot
        fig, ax = plt.subplots(figsize=(6, 5), dpi=100)
        
        try:
            # Calculate partial dependence (optimized)
            feature_idx = X.columns.get_loc(selected_pd_feature)
            
            # Generate PDP with optimized settings
            display = PartialDependenceDisplay.from_estimator(
                _rf_results['model'], 
                X, 
                features=[feature_idx],
                kind='both',  # Shows both line and ICE
                centered=True, 
                ax=ax,
                grid_resolution=50  # Avoid multiprocessing overhead for small data
            )

            # Customize plot
            ax.set_title(f'Partial Dependence: {selected_pd_feature}', fontsize=11)
            ax.set_xlabel(f'Feature: {selected_pd_feature}', fontsize=10)
            ax.set_ylabel('Partial Dependence', fontsize=10)
            ax.grid(True, alpha=0.3)

            plt.tight_layout()
            st.pyplot(fig, use_container_width=True)
            plt.close(fig)  # Explicit cleanup
            
        except Exception as e:
            st.error(f"Could not create partial dependence plot: {str(e)}")
            plt.close(fig)
    
    with col2:
        # Feature distribution (optimized)
        fig, ax = plt.subplots(figsize=(6, 5), dpi=100)
        
        feature_data = X[selected_pd_feature]
        
        # Optimize histogram bins based on data size
        n_bins = min(30, max(10, len(feature_data) // 20))
        
        # Create histogram
        ax.hist(feature_data, bins=n_bins, alpha=0.7, color='skyblue', edgecolor='black')
        ax.set_xlabel(f'Feature: {selected_pd_feature}', fontsize=10)
        ax.set_ylabel('Frequency', fontsize=10)
        ax.set_title(f'Distribution: {selected_pd_feature}', fontsize=11)
        ax.grid(True, alpha=0.3)
        
        # Add statistics (pre-computed for performance)
        mean_val = feature_data.mean()
        median_val = feature_data.median()
        
        ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, 
                  label=f'Mean: {mean_val:.3f}')
        ax.axvline(median_val, color='orange', linestyle='--', linewidth=2, 
                  label=f'Median: {median_val:.3f}')
        ax.legend(fontsize=9)
        
        plt.tight_layout()
        st.pyplot(fig, use_container_width=True)
        plt.close(fig)  # Explicit cleanup
    
    # Feature statistics (optimized DataFrame creation)
    st.write(f"**Statistics for {selected_pd_feature}:**")
    col1, col2 = st.columns(2)
    
    with col1:
        # Pre-compute all statistics at once
        feature_data = X[selected_pd_feature]
        stats_dict = {
            'Count': feature_data.count(),
            'Mean': feature_data.mean(),
            'Std': feature_data.std(),
            'Min': feature_data.min(),
            '25%': feature_data.quantile(0.25),
            '50%': feature_data.quantile(0.50),
            '75%': feature_data.quantile(0.75),
            'Max': feature_data.max(),
            'Non-Zero Count': (feature_data != 0).sum()
        }
        
        stats_df = pd.DataFrame({
            'Statistic': list(stats_dict.keys()),
            'Value': list(stats_dict.values())
        })
        
        st.dataframe(
            stats_df.style.format({'Value': '{:.4f}'}),
            use_container_width=True
        )

# Cached utility functions for analysis results display
@st.cache_data
def get_cached_results(_analysis_results):
    """Return cached analysis results for display"""
    return _analysis_results

@st.cache_data
def compute_correlation_matrix(X_data):
    """Compute correlation matrix with caching"""
    return X_data.corr()

@st.cache_data  
def compute_target_correlations(X_data, y_data):
    """Compute correlations with target variable with caching"""
    return X_data.corrwith(y_data).abs().sort_values(ascending=False)

# Streamlit App
st.markdown("<h1 class='main-header'>ðŸ§ª Chemical Data Analysis Tool</h1>", unsafe_allow_html=True)

st.sidebar.header("ðŸ“ Upload & Configure")
uploaded_file = st.sidebar.file_uploader("Upload CSV or Excel", type=['csv', 'xlsx', 'xls'])

def main():
    if uploaded_file is not None:
        # Check if a new file has been uploaded and clear previous analysis
        if 'uploaded_file_name' not in st.session_state or st.session_state.uploaded_file_name != uploaded_file.name:
            # New file detected - clear previous analysis
            if st.session_state.get('analysis_complete', False):
                st.info("ðŸ”„ New dataset detected. Previous analysis has been cleared.")
            
            # Clear all analysis-related session state
            keys_to_clear = []
            for key in st.session_state.keys():
                if key.startswith(('analysis_', 'individual_feature', 'pd_feature')):
                    keys_to_clear.append(key)
            
            for key in keys_to_clear:
                del st.session_state[key]
            
            # Clear Streamlit cache to ensure fresh analysis for new file
            st.cache_data.clear()
            
            st.session_state.analysis_complete = False
            st.session_state.analysis_results = {}
            st.session_state.uploaded_file_name = uploaded_file.name
        
        try:
            # Load data
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            else:
                excel_file = pd.ExcelFile(uploaded_file)
                if len(excel_file.sheet_names) > 1:
                    selected_sheet = st.sidebar.selectbox("Select Sheet", excel_file.sheet_names, key="sheet_selector")
                else:
                    selected_sheet = excel_file.sheet_names[0]
                df = pd.read_excel(uploaded_file, sheet_name=selected_sheet)
            
            st.success(f"Data loaded: {df.shape}")
            
            # Data preview
            with st.expander("ðŸ“Š Data Preview"):
                st.dataframe(df)
            
            # Target selection
            numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
            if not numeric_columns:
                st.error("No numeric columns found!")
                return
            
            target_column = st.sidebar.selectbox("ðŸŽ¯ Select Target", numeric_columns, key="target_column_selector")
            
            # Reference column selection
            st.sidebar.subheader("ðŸ”¬ Reference Filtering (Optional)")
            st.sidebar.write("Filter data by experimental reference/batch")
            
            all_columns = df.columns.tolist()
            reference_options = ["None (use all data)"] + all_columns
            
            reference_column = st.sidebar.selectbox(
                "Reference Column", 
                reference_options,
                help="Select a column that contains experimental references/batches",
                key="reference_column_selector"
            )
            
            reference_value = None
            if reference_column != "None (use all data)":
                unique_refs = df[reference_column].dropna().unique()
                if len(unique_refs) >= 1:
                    reference_value = st.sidebar.selectbox(
                        "Select Reference Value",
                        unique_refs,
                        help="Choose which reference/batch to analyze",
                        key="reference_value_selector"
                    )
                    
                    matching_rows = df[df[reference_column] == reference_value].shape[0]
                    st.sidebar.info(f"ðŸ“Š {matching_rows} rows match this reference")
                else:
                    st.sidebar.warning("No valid reference values found in this column")
                    reference_column = "None (use all data)"
            
            # Column Exclusion Options
            st.sidebar.subheader("ðŸš« Column Exclusions (Optional)")
            st.sidebar.write("Manually exclude columns from analysis")
            
            # Get all columns except the selected target
            available_for_exclusion = [col for col in all_columns if col != target_column]
            
            # Add reference column info if one is selected
            if reference_column != "None (use all data)":
                available_for_exclusion = [col for col in available_for_exclusion if col != reference_column]
                st.sidebar.info(f"â„¹ï¸ Target '{target_column}' and reference '{reference_column}' are automatically excluded from selection")
            else:
                st.sidebar.info(f"â„¹ï¸ Target '{target_column}' is automatically excluded from selection")
            
            excluded_columns = st.sidebar.multiselect(
                "Select columns to exclude:",
                available_for_exclusion,
                default=[],
                help="These columns will be removed before analysis (in addition to automatic preprocessing)",
                key="excluded_columns_multiselect"
            )
            
            if excluded_columns:
                st.sidebar.success(f"âœ… Will exclude {len(excluded_columns)} column(s)")
                with st.sidebar.expander("ðŸ“‹ Columns to exclude"):
                    for col in excluded_columns:
                        st.sidebar.write(f"â€¢ {col}")
            
            # Settings
            missing_threshold = st.sidebar.slider("Missing Value Threshold", 0.5, 1.0, 0.95, 0.05, key="missing_threshold_slider")
            
            # Random Forest Settings
            st.sidebar.subheader("ðŸŒ² Random Forest Settings")
            
            use_train_test = st.sidebar.checkbox(
                "Use Train/Test Split", 
                value=False,
                help="For small datasets, it's often better to use all data for training",
                key="use_train_test_checkbox"
            )
            
            use_grid_search = st.sidebar.checkbox(
                "Use Grid Search", 
                value=False,
                help="Optimize Random Forest hyperparameters",
                key="use_grid_search_checkbox"
            )
            
            param_grid = None
            if use_grid_search:
                st.sidebar.write("**Grid Search Parameters:**")
                
                # Configure grid search parameters
                n_estimators_options = st.sidebar.multiselect(
                    "Number of Trees", 
                    [10, 20, 30, 50, 100, 200, 300, 500],
                    default=[10, 20, 30, 50, 100, 200],
                    key="n_estimators_multiselect"
                )
                
                max_depth_options = st.sidebar.multiselect(
                    "Max Depth", 
                    [None, 3, 5, 10, 15, 20],
                    default=[None, 3, 5, 10, 15, 20],
                    key="max_depth_multiselect"
                )
                
                min_samples_split_options = st.sidebar.multiselect(
                    "Min Samples Split",
                    [2, 5, 10, 20],
                    default=[2, 5, 10],
                    key="min_samples_split_multiselect"
                )
                
                min_samples_leaf_options = st.sidebar.multiselect(
                    "Min Samples Leaf",
                    [1, 2, 4, 8],
                    default=[1, 2, 4, 8],
                    key="min_samples_leaf_multiselect"
                )
                
                max_features_options = st.sidebar.multiselect(
                    "Max Features",
                    ['sqrt', 'log2', 0.8, 0.6, 0.4, 0.2, None],
                    default=['sqrt', 'log2', 0.8, 0.6],
                    key="max_features_multiselect"
                )
                
                if n_estimators_options:
                    param_grid = {
                        'n_estimators': n_estimators_options,
                        'max_depth': max_depth_options,
                        'min_samples_split': min_samples_split_options,
                        'min_samples_leaf': min_samples_leaf_options,
                        'max_features': max_features_options
                    }
                else:
                    st.sidebar.warning("Select at least one parameter for grid search!")
            
            if st.sidebar.button("ðŸš€ Run Analysis", type="primary"):
                # Reset session state for new analysis
                st.session_state.analysis_complete = False
                st.session_state.analysis_results = {}
                st.session_state.main_results_displayed = False
                
                # Clear interactive component states too
                if "selected_individual_feature" in st.session_state:
                    del st.session_state.selected_individual_feature
                if "selected_pd_feature" in st.session_state:
                    del st.session_state.selected_pd_feature
                
                # Apply reference filtering first
                df_filtered = df.copy()
                if reference_column != "None (use all data)" and reference_value is not None:
                    df_filtered = df_filtered[df_filtered[reference_column] == reference_value]
                    st.info(f"ðŸ”¬ Analyzing data for reference: **{reference_value}** ({df_filtered.shape[0]} rows)")
                else:
                    st.info(f"ðŸ“Š Analyzing all data ({df_filtered.shape[0]} rows)")
                
                if df_filtered.empty:
                    st.error("No data remaining after reference filtering!")
                    return
                
                # Preprocessing with optimized function
                st.subheader("ðŸ§¹ Data Preprocessing")
                
                with st.spinner("Preprocessing data..."):
                    # Use optimized preprocessing function
                    df_processed, preprocessing_metadata = preprocess_data(
                        df_filtered, target_column, missing_threshold
                    )
                    
                    if target_column not in df_processed.columns:
                        st.error(f"Target column '{target_column}' was removed during preprocessing!")
                        return
                    
                    # Extract data from optimized metadata
                    original_shape = preprocessing_metadata['original_shape']
                    processed_shape = preprocessing_metadata['final_shape']
                    original_missing = preprocessing_metadata['original_missing_total']
                    processed_missing = preprocessing_metadata['final_missing_total']
                    original_duplicates = preprocessing_metadata['original_duplicates']
                    processed_duplicates = preprocessing_metadata['final_duplicates']
                    removed_columns = preprocessing_metadata['removed_columns']
                    
                    rows_removed = original_shape[0] - processed_shape[0]
                    columns_removed = original_shape[1] - processed_shape[1]
                    
                    # Remove reference column from analysis if it was used for filtering
                    columns_to_exclude = [target_column]
                    if reference_column != "None (use all data)" and reference_column in df_processed.columns:
                        columns_to_exclude.append(reference_column)
                    
                    # Add user-selected excluded columns
                    if excluded_columns:
                        # Only add columns that still exist in the processed dataframe
                        existing_excluded = [col for col in excluded_columns if col in df_processed.columns]
                        columns_to_exclude.extend(existing_excluded)
                        
                        if existing_excluded:
                            st.info(f"ðŸš« Manually excluding {len(existing_excluded)} user-selected column(s): {', '.join(existing_excluded)}")
                    
                    # Optimized feature selection
                    X = df_processed.drop(columns=columns_to_exclude, errors='ignore')
                    y = df_processed[target_column]
                    X = X.select_dtypes(include=[np.number])
                    
                    # Final feature count after selecting numeric columns only
                    numeric_features_count = X.shape[1]
                
                    # Create detailed column status using metadata
                    column_status_data = []
                    
                    for col in preprocessing_metadata['original_columns']:
                        col_info = preprocessing_metadata['column_analysis'][col]
                        missing_pct = col_info['missing_pct'] * 100
                        
                        if col in preprocessing_metadata['final_columns']:
                            # Column was kept during preprocessing
                            if col == target_column:
                                status = "Target Variable"
                                action = "Used as target"
                            elif col == reference_column and reference_column != "None (use all data)":
                                status = "Reference Column"
                                action = "Used for filtering"
                            elif excluded_columns and col in excluded_columns:
                                status = "Manually Excluded"
                                action = "User-selected for exclusion"
                            elif col in X.columns:
                                status = "Feature (Kept)"
                                action = f"Available for analysis" + (f", {col_info['missing_count']} missing values filled" if col_info['missing_count'] > 0 else "")
                            else:
                                status = "Non-numeric"
                                action = "Excluded from analysis (not numeric)"
                        else:
                            # Column was removed
                            if missing_pct > missing_threshold * 100:
                                status = "Removed"
                                action = f"Too many missing values ({missing_pct:.1f}%)"
                            elif col == '1':
                                status = "Removed"
                                action = "Main peak column excluded"
                            else:
                                status = "Removed"
                                action = "Other preprocessing rule"
                        
                        column_status_data.append({
                            'Column': col,
                            'Status': status,
                            'Action': action,
                            'Missing %': f"{missing_pct:.1f}%"
                        })
                    
                    column_status_df = pd.DataFrame(column_status_data)
                    
                    # Create consolidated preprocessing results
                    preprocessing_results = {
                        'original_shape': original_shape,
                        'final_shape': processed_shape,
                        'target_column': target_column,
                        'rows_removed': rows_removed,
                        'columns_removed': columns_removed,
                        'duplicates_removed': original_duplicates - processed_duplicates,
                        'missing_data_summary': pd.DataFrame({
                            'Column': df_processed.columns,
                            'Missing Count': df_processed.isnull().sum().values,
                            'Total Count': len(df_processed)
                        }),
                        'removed_columns': list(removed_columns),
                        'numeric_features_count': numeric_features_count,
                        'column_status_df': column_status_df,
                        'metadata': preprocessing_metadata  # Include full metadata
                    }
                
                if X.empty:
                    st.error("No numeric features available for analysis!")
                    return
                
                # Display Preprocessing Results
                st.subheader("ðŸ“‹ Preprocessing Results Summary")
                
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric(
                        "Rows", 
                        f"{processed_shape[0]:,}", 
                        delta=f"{-rows_removed:,}" if rows_removed > 0 else "No change",
                        help=f"Original: {original_shape[0]:,} rows"
                    )
                
                with col2:
                    st.metric(
                        "Total Columns", 
                        f"{processed_shape[1]:,}", 
                        delta=f"{-columns_removed:,}" if columns_removed > 0 else "No change",
                        help=f"Original: {original_shape[1]:,} columns"
                    )
                
                with col3:
                    st.metric(
                        "Numeric Features", 
                        f"{numeric_features_count:,}",
                        help="Features available for analysis (excluding target and reference columns)"
                    )
                
                with col4:
                    missing_change = original_missing - processed_missing
                    st.metric(
                        "Missing Values", 
                        f"{processed_missing:,}", 
                        delta=f"-{missing_change:,}" if missing_change > 0 else "No change",
                        help=f"Original: {original_missing:,} missing values"
                    )
                
                # Detailed preprocessing information
                with st.expander("ðŸ” Detailed Preprocessing Information", expanded=False):
                    
                    # Data shape changes
                    st.write("**Data Shape Changes:**")
                    shape_df = pd.DataFrame({
                        'Stage': ['Original Data', 'After Preprocessing'],
                        'Rows': [f"{original_shape[0]:,}", f"{processed_shape[0]:,}"],
                        'Columns': [f"{original_shape[1]:,}", f"{processed_shape[1]:,}"],
                        'Missing Values': [f"{original_missing:,}", f"{processed_missing:,}"],
                        'Duplicate Rows': [f"{original_duplicates:,}", f"{processed_duplicates:,}"]
                    })
                    st.dataframe(shape_df, use_container_width=True)
                    
                    # Removed columns analysis
                    if removed_columns:
                        st.write(f"**Columns Removed ({len(removed_columns)}):**")
                        
                        # Analyze why columns were removed
                        removal_reasons = []
                        for col in removed_columns:
                            if col in df_filtered.columns:
                                missing_pct = df_filtered[col].isnull().sum() / len(df_filtered)
                                if missing_pct > missing_threshold:
                                    removal_reasons.append({
                                        'Column': col,
                                        'Reason': f'Too many missing values ({missing_pct:.1%})',
                                        'Missing %': f"{missing_pct:.1%}"
                                    })
                                elif col == '1':
                                    removal_reasons.append({
                                        'Column': col,
                                        'Reason': 'main peak removed',
                                        'Missing %': f"{missing_pct:.1%}"
                                    })
                                else:
                                    removal_reasons.append({
                                        'Column': col,
                                        'Reason': 'Other preprocessing rule',
                                        'Missing %': f"{missing_pct:.1%}"
                                    })
                        
                        if removal_reasons:
                            removal_df = pd.DataFrame(removal_reasons)
                            st.dataframe(removal_df, use_container_width=True)
                    
                    # Missing values handling
                    if original_missing > 0:
                        st.write("**Missing Values Handling:**")
                        
                        # Calculate missing values by column before and after
                        original_missing_by_col = df_filtered.isnull().sum()
                        processed_missing_by_col = df_processed.isnull().sum()
                        
                        # Show columns that had missing values
                        missing_cols = original_missing_by_col[original_missing_by_col > 0]
                        if len(missing_cols) > 0:
                            missing_summary = pd.DataFrame({
                                'Column': missing_cols.index,
                                'Original Missing': missing_cols.values,
                                'Original Missing %': (missing_cols / len(df_filtered) * 100).round(2),
                                'After Processing': [processed_missing_by_col.get(col, 0) for col in missing_cols.index],
                                'Action Taken': ['Filled with 0' if col in processed_missing_by_col.index else 'Column removed' 
                                               for col in missing_cols.index]
                            })
                            st.dataframe(missing_summary.head(20), use_container_width=True)
                            
                            if len(missing_summary) > 20:
                                st.info(f"Showing top 20 of {len(missing_summary)} columns with missing values")
                    
                    # Duplicate rows handling
                    if original_duplicates > 0:
                        st.write("**Duplicate Rows:**")
                        duplicate_df = pd.DataFrame({
                            'Metric': ['Original duplicate rows', 'Duplicate rows removed', 'Final duplicate rows'],
                            'Count': [original_duplicates, original_duplicates - processed_duplicates, processed_duplicates]
                        })
                        st.dataframe(duplicate_df, use_container_width=True)
                    
                    # Feature type analysis
                    st.write("**Feature Types Analysis:**")
                    
                    # Analyze data types in processed data
                    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()
                    categorical_cols = df_processed.select_dtypes(include=['object', 'category']).columns.tolist()
                    datetime_cols = df_processed.select_dtypes(include=['datetime']).columns.tolist()
                    boolean_cols = df_processed.select_dtypes(include=['bool']).columns.tolist()
                    
                    feature_types_df = pd.DataFrame({
                        'Data Type': ['Numeric', 'Categorical', 'DateTime', 'Boolean'],
                        'Count': [len(numeric_cols), len(categorical_cols), len(datetime_cols), len(boolean_cols)],
                        'Available for Analysis': [
                            len([col for col in numeric_cols if col not in columns_to_exclude]),
                            0,  # Categorical not used in this analysis
                            0,  # DateTime not used in this analysis  
                            0   # Boolean not used in this analysis
                        ]
                    })
                    st.dataframe(feature_types_df, use_container_width=True)
                
                st.success(f"âœ… Preprocessing complete! Ready to analyze {numeric_features_count:,} numeric features from {processed_shape[0]:,} samples.")
                
                # Store processed data in session state
                st.session_state.analysis_results = {
                    'df_original': df,
                    'df_filtered': df_filtered,
                    'df_processed': df_processed,
                    'X': X,
                    'y': y,
                    'target_column': target_column,
                    'reference_column': reference_column,
                    'reference_value': reference_value,
                    'missing_threshold': missing_threshold,
                    'excluded_columns': excluded_columns,
                    'use_train_test': use_train_test,
                    'use_grid_search': use_grid_search,
                    'param_grid': param_grid,
                    'preprocessing_results': preprocessing_results
                }
                
                # Run Univariate Regression
                with st.spinner("Running univariate regression analysis..."):
                    univariate_results, fitted_models = perform_univariate_regression(X, y)
                    
                    # Store univariate results
                    st.session_state.analysis_results.update({
                        'univariate_results': univariate_results,
                        'fitted_models': fitted_models,
                        'rf_complete': False
                    })
                
                # Run Random Forest
                with st.spinner("Training Random Forest model..."):
                    rf_results = train_random_forest(X, y, use_train_test, param_grid)
                    
                    # Store RF results
                    st.session_state.analysis_results.update({
                        'rf_results': rf_results,
                        'rf_complete': True
                    })
                
                # Mark analysis as complete
                st.session_state.analysis_complete = True
                st.rerun()  # Force rerun to display results immediately
                
        except Exception as e:
            # Error handling for the main analysis workflow
            st.error(f"âŒ Analysis Error: {str(e)}")
            st.error("Please check your data and try again. Common issues:")
            st.markdown("""
            - **Data format**: Ensure your file is a valid CSV or Excel file
            - **Target column**: Make sure the selected target column contains numeric values
            - **Missing data**: Too many missing values might cause analysis to fail
            - **Small dataset**: Very small datasets might not work with train/test split
            """)
            
            # Clear session state on error to allow fresh restart
            st.session_state.analysis_complete = False
            if 'analysis_results' in st.session_state:
                del st.session_state.analysis_results
            
            # Provide debug information in an expander
            with st.expander("ðŸ”§ Debug Information"):
                st.code(str(e))

    # Persistent results display (optimized with caching)
    if st.session_state.get('analysis_complete', False) and 'analysis_results' in st.session_state:
        
        # Use cached results to avoid recomputation
        results = get_cached_results(st.session_state.analysis_results)
        
        # Ensure all required data exists
        required_keys = ['X', 'y', 'univariate_results', 'fitted_models']
        if results and all(key in results for key in required_keys):
            X = results['X']
            y = results['y']
            univariate_results = results['univariate_results']
            fitted_models = results['fitted_models']
            rf_results = results.get('rf_results', None)
            rf_complete = results.get('rf_complete', False)
            target_column = results['target_column']
            use_train_test = results.get('use_train_test', False)
            use_grid_search = results.get('use_grid_search', False)
            
            # Data preprocessing display (in expander but expanded by default)
            if 'preprocessing_results' in results:
                with st.expander("ðŸ“Š Data Preprocessing Results", expanded=True):
                    preprocessing = results['preprocessing_results']
                    
                    # Key metrics in columns
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("  Number of Samples", preprocessing['original_shape'][0])
                    with col2:
                        st.metric("ðŸ“ Original Columns", preprocessing['original_shape'][1])
                    with col3:
                        st.metric("ðŸ“‹ Columns After Preprocessing", preprocessing['final_shape'][1])
                    with col4:
                        st.metric("ðŸ”¢ Numeric Features", preprocessing['numeric_features_count'])
                    
                    # Show what happened to each column
                    st.subheader("ðŸ“‹ Column Processing Details")
                    st.write("Complete breakdown of what happened to each column during preprocessing:")
                    
                    # Display the detailed column status table
                    if 'column_status_df' in preprocessing:
                        column_status = preprocessing['column_status_df']
                        
                        # Style the dataframe to highlight different statuses
                        def style_status(val):
                            if val == "Target Variable":
                                return "background-color: #e8f5e8; color: #2d5a2d"
                            elif val == "Feature (Kept)":
                                return "background-color: #e8f4fd; color: #1f4e79"
                            elif val == "Reference Column":
                                return "background-color: #fff4e6; color: #8b4513"
                            elif val == "Manually Excluded":
                                return "background-color: #ffe6f0; color: #8b2252"
                            elif val == "Removed":
                                return "background-color: #ffeaea; color: #8b0000"
                            elif val == "Non-numeric":
                                return "background-color: #f0f0f0; color: #666666"
                            return ""
                        
                        styled_df = column_status.style.applymap(style_status, subset=['Status'])
                        st.dataframe(styled_df, use_container_width=True)
                        
                        # Summary statistics
                        st.subheader("ðŸ“Š Processing Summary")
                        
                        status_counts = column_status['Status'].value_counts()
                        
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            kept_features = status_counts.get('Feature (Kept)', 0)
                            st.metric("âœ… Features Kept", kept_features)
                        with col2:
                            removed_cols = status_counts.get('Removed', 0)
                            st.metric("âŒ Auto Removed", removed_cols)
                        with col3:
                            manually_excluded = status_counts.get('Manually Excluded', 0)
                            st.metric("ðŸš« User Excluded", manually_excluded)
                        with col4:
                            non_numeric = status_counts.get('Non-numeric', 0)
                            st.metric("ðŸ“ Non-numeric", non_numeric)
                    
                    # Additional information if duplicates were removed
                    if preprocessing['duplicates_removed'] > 0:
                        st.info(f"ðŸ”„ Removed {preprocessing['duplicates_removed']} duplicate rows")
                    
                    # Check if there are missing values that were handled
                    missing_df = preprocessing['missing_data_summary']
                    total_missing = missing_df['Missing Count'].sum()
                    
                    if total_missing > 0:
                        with st.expander(" ï¸ Missing Data Details", expanded=False):
                            st.write("Columns with missing data and how they were handled:")
                            missing_df_display = missing_df[missing_df['Missing Count'] > 0].copy()
                            missing_df_display['Percentage'] = (missing_df_display['Missing Count'] / missing_df_display['Total Count'] * 100).round(2)
                            st.dataframe(missing_df_display, use_container_width=True)
            
            # Display main analysis results
            st.subheader("ðŸ“ˆ Univariate Regression Analysis")
            significance_threshold = st.selectbox(
                "Significance Level", [0.001, 0.01, 0.05, 0.1], index=2
            )
            
            significant_results = univariate_results[
                univariate_results['p_value'] <= significance_threshold
            ].sort_values('p_value')
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Total Features", len(univariate_results))
            with col2:
                st.metric("Significant Features", len(significant_results))
            
            # Results table
            st.dataframe(
                significant_results.style.format({
                    'Coefficient': '{:.4f}',
                    'p_value': '{:.2e}',
                    'R_squared': '{:.4f}',
                    'RMSE': '{:.4f}'
                }),
                use_container_width=True
            )
            
            # Correlation Analysis (optimized with caching)
            st.subheader("ðŸ”— Correlation Analysis")
            
            col1, col2 = st.columns(2)
            
            # Preprocessed Data Correlation Heatmap
            with col1:
                st.write("**All Features Correlation Heatmap**")
                
                # Use optimized figure size and settings
                fig, ax = plt.subplots(figsize=(8, 6), dpi=100)
                
                # Calculate correlation matrix (cached)
                correlation_matrix = compute_correlation_matrix(X)
                
                # Create optimized heatmap
                sns.heatmap(
                    correlation_matrix, 
                    annot=False,  # No annotations for performance
                    cmap='coolwarm', 
                    center=0,
                    square=True,
                    ax=ax,
                    cbar_kws={"shrink": 0.8},
                    xticklabels=True,  # Show labels for readability
                    yticklabels=True
                )
                ax.set_title('Feature Correlation Matrix (All Features)', fontsize=11)
                plt.tight_layout()
                st.pyplot(fig, use_container_width=True)
                plt.close(fig)
            
            # Significant Features Correlation Heatmap
            with col2:
                st.write("**Significant Features Correlation Heatmap**")
                
                if len(significant_results) > 1:
                    # Get significant feature names
                    significant_features = significant_results['Feature'].tolist()
                    
                    # Limit to top features for performance
                    max_features = min(15, len(significant_features))
                    significant_features = significant_features[:max_features]
                    
                    # Create correlation matrix for significant features only
                    significant_X = X[significant_features]
                    significant_corr = compute_correlation_matrix(significant_X)
                    
                    fig, ax = plt.subplots(figsize=(8, 6), dpi=100)
                    sns.heatmap(
                        significant_corr, 
                        annot=True, 
                        cmap='coolwarm', 
                        center=0,
                        square=True,
                        ax=ax,
                        fmt='.2f',
                        cbar_kws={"shrink": 0.8},
                        xticklabels=True,
                        yticklabels=True
                    )
                    ax.set_title(f'Significant Features Correlation\n(p â‰¤ {significance_threshold})', fontsize=11)
                    plt.xticks(rotation=45, ha='right', fontsize=8)
                    plt.yticks(rotation=0, fontsize=8)
                    plt.tight_layout()
                    st.pyplot(fig, use_container_width=True)
                    plt.close(fig)
                else:
                    st.info("Need at least 2 significant features for correlation analysis")
            
            # Correlation with target (optimized)
            st.write("**Target Correlation Analysis**")
            col1, col2 = st.columns(2)
            
            with col1:
                # Calculate correlation with target variable (cached)
                target_correlations = compute_target_correlations(X, y)
                
                # Display top correlations
                top_corr_features = target_correlations.head(15)
                
                fig, ax = plt.subplots(figsize=(10, 6), dpi=100)
                
                # Create horizontal bar plot with optimized settings
                y_pos = np.arange(len(top_corr_features))
                bars = ax.barh(y_pos, top_corr_features.values, color='steelblue', alpha=0.7)
                
                ax.set_yticks(y_pos)
                ax.set_yticklabels(top_corr_features.index, fontsize=9)
                ax.set_xlabel('Absolute Correlation with Target', fontsize=10)
                ax.set_title(f'Top 15 Features by Correlation with {target_column}', fontsize=11)
                ax.invert_yaxis()
                
                # Add value labels on bars (optimized)
                for i, (bar, value) in enumerate(zip(bars, top_corr_features.values)):
                    ax.text(value + 0.01, bar.get_y() + bar.get_height()/2, 
                        f'{value:.3f}', ha='left', va='center', fontsize=8)
                
                plt.tight_layout()
                st.pyplot(fig, use_container_width=True)
                plt.close(fig)
            
            # Random Forest Analysis (only if RF is complete)
            if rf_results and rf_complete:
                st.subheader("ðŸŒ² Random Forest Analysis")
                
                # Display Random Forest results
                analysis_method = "Train/Test Split" if use_train_test else "Full Dataset"
                optimization_method = "Grid Search" if use_grid_search else "Default Parameters"
                st.info(f"ðŸ”§ Method: {analysis_method} | Optimization: {optimization_method}")
                
                # Performance metrics
                col1, col2, col3, col4 = st.columns(4)
                
                if use_train_test:
                    with col1:
                        st.metric("Test RÂ²", f"{rf_results['test_r2']:.4f}")
                    with col2:
                        st.metric("Test RMSE", f"{rf_results['test_rmse']:.4f}")
                else:
                    with col1:
                        st.metric("Full RÂ²", f"{rf_results['full_r2']:.2f}")
                    with col2:
                        st.metric("Full RMSE", f"{rf_results['full_rmse']:.4f}")
                
                # Visualizations
                st.write("ðŸ“Š **Visualizations**")
                
                col1, col2 = st.columns(2)
                
                # Predicted vs Actual
                with col1:
                    st.write("ðŸŽ¯ **Predicted vs Actual**")
                    fig, ax = plt.subplots(figsize=(6, 5))
                    
                    if use_train_test:
                        ax.scatter(rf_results['y_test'], rf_results['y_pred_test'], alpha=0.6)
                        min_val = min(rf_results['y_test'].min(), rf_results['y_pred_test'].min())
                        max_val = max(rf_results['y_test'].max(), rf_results['y_pred_test'].max())
                        ax.set_title('Random Forest: Test Set Predictions')
                    else:
                        ax.scatter(rf_results['y_actual'], rf_results['y_pred_full'], alpha=0.6, color='teal')
                        min_val = min(rf_results['y_actual'].min(), rf_results['y_pred_full'].min())
                        max_val = max(rf_results['y_actual'].max(), rf_results['y_pred_full'].max())
                        ax.set_title('Random Forest: Full Dataset Predictions')

                    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, color='gray')
                    ax.set_xlabel('Actual Values')
                    ax.set_ylabel('Predicted Values')
                    st.pyplot(fig)
                    plt.close()
                
                # Feature Importance - rfpimp Permutation Importance
                with col2:
                    st.write("â­ **rfpimp Permutation Feature Importance**")
                    
                    # Check if rfpimp is available and get the appropriate importance data
                    if rf_results['rfpimp_available'] and 'grouped_importance' in rf_results:
                        # Use rfpimp-based grouped importance (which is actually rfpimp permutation importance)
                        importance_data = rf_results['grouped_importance'].head(15)  # Show top 15
                        importance_col = 'Grouped_Importance'
                        title = 'rfpimp Permutation Feature Importance'
                        subtitle = 'âœ… Using rfpimp package for accurate permutation importance'
                        
                    else:
                        # Fallback to sklearn permutation importance
                        importance_data = rf_results['permutation_importance'].head(15)  # Show top 15
                        importance_col = 'Permutation_Importance'
                        title = 'Permutation Feature Importance (Fallback)'
                        if 'rfpimp_error' in rf_results:
                            subtitle = f'âš ï¸ rfpimp error: {rf_results["rfpimp_error"][:50]}... Using sklearn fallback'
                        else:
                            subtitle = 'â„¹ï¸ rfpimp not available. Using sklearn permutation importance'
                    
                    # Create single horizontal bar plot
                    fig, ax = plt.subplots(figsize=(10, 8), dpi=100)
                    
                    # Create horizontal bar plot
                    y_pos = np.arange(len(importance_data))
                    bars = ax.barh(y_pos, importance_data[importance_col], 
                                  color='steelblue', alpha=0.7, edgecolor='navy', linewidth=0.5)
                    
                    # Customize the plot
                    ax.set_yticks(y_pos)
                    ax.set_yticklabels(importance_data['Feature'], fontsize=9)
                    ax.set_xlabel('Importance Score', fontsize=10)
                    ax.set_title(title, fontsize=12, fontweight='bold', pad=20)
                    ax.invert_yaxis()  # Highest importance at top
                    
                    # Add value labels on bars
                    for i, (bar, value) in enumerate(zip(bars, importance_data[importance_col])):
                        ax.text(value + max(importance_data[importance_col]) * 0.01, 
                               bar.get_y() + bar.get_height()/2, 
                               f'{value:.4f}', ha='left', va='center', fontsize=8)
                    
                    # Add grid for better readability
                    ax.grid(True, axis='x', alpha=0.3, linestyle='--')
                    
                    # Adjust layout
                    plt.tight_layout()
                    st.pyplot(fig, use_container_width=True)
                    plt.close(fig)  # Explicit cleanup
                    
                    # Status message below the plot
                    if rf_results['rfpimp_available']:
                        st.success(subtitle)
                    else:
                        if 'rfpimp_error' in rf_results:
                            st.warning(subtitle)
                        else:
                            st.info(subtitle + " Install with: `pip install rfpimp` for better accuracy.")
                
                # Remove the old rfpimp status information section (now integrated above)
            
            # Interactive Features Section
            st.subheader("ðŸ” Interactive Analysis")
            
            # Individual Feature Analysis
            st.write("**Individual Feature Analysis:**")
            available_features = univariate_results['Feature'].tolist()
            
            # Initialize default selection if not in session state
            if "individual_feature_selection" not in st.session_state:
                st.session_state.individual_feature_selection = available_features[0] if available_features else None
            
            selected_feature = st.selectbox(
                "Select feature for detailed analysis:",
                available_features,
                index=available_features.index(st.session_state.individual_feature_selection) if st.session_state.individual_feature_selection in available_features else 0,
                key="individual_feature_selection",
                help="Select any feature to see detailed regression analysis"
            )
            
            if selected_feature:
                create_individual_analysis(selected_feature, univariate_results, fitted_models, y)
            
            # Partial Dependence Plot (only if RF is complete)
            if rf_results and rf_complete:
                st.write("ðŸ“Š **Interactive Partial Dependence Plot**")
                st.write("Visualize how a single feature affects model predictions while holding others constant")
                
                feature_options = X.columns.tolist()
                
                # Initialize default selection if not in session state
                if "pd_feature_selection" not in st.session_state:
                    st.session_state.pd_feature_selection = feature_options[0] if feature_options else None
                
                selected_pd_feature = st.selectbox(
                    "Select feature for partial dependence plot:",
                    feature_options,
                    index=feature_options.index(st.session_state.pd_feature_selection) if st.session_state.pd_feature_selection in feature_options else 0,
                    help="Shows how this feature affects predictions when other features are held at their average values",
                    key="pd_feature_selection"
                )
                
                if selected_pd_feature:
                    create_partial_dependence_plot(selected_pd_feature, rf_results, X)
            else:
                st.info("ðŸŒ² Complete Random Forest analysis to access Partial Dependence Plots")
            
            # Download results section (moved to bottom)
            st.subheader("ðŸ’¾ Download Results")
            
            univariate_csv = univariate_results.to_csv(index=False)
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.download_button(
                    "ðŸ“¥ Download Univariate Results",
                    data=univariate_csv,
                    file_name="univariate_regression_results.csv",
                    mime="text/csv"
                )
            
            if rf_results and rf_complete:
                permutation_importance_csv = rf_results['permutation_importance'].to_csv(index=False)
                default_importance_csv = rf_results['default_importance'].to_csv(index=False)
                grouped_importance_csv = rf_results['grouped_importance'].to_csv(index=False)
                
                with col2:
                    st.download_button(
                        "ðŸ“¥ Download Permutation Importance",
                        data=permutation_importance_csv,
                        file_name="permutation_feature_importance.csv",
                        mime="text/csv"
                    )
                
                with col3:
                    st.download_button(
                        "ðŸ“¥ Download Default Importance",
                        data=default_importance_csv,
                        file_name="default_feature_importance.csv",
                        mime="text/csv"
                    )
                
                with col4:
                    importance_type = "rfpimp" if rf_results['rfpimp_available'] else "fallback"
                    st.download_button(
                        "ðŸ“¥ Download Grouped Importance",
                        data=grouped_importance_csv,
                        file_name=f"grouped_feature_importance_{importance_type}.csv",
                        mime="text/csv"
                    )
            else:
                with col2:
                    st.info("Complete RF analysis for more downloads")
                with col3:
                    st.info("Complete RF analysis for more downloads")
                with col4:
                    st.info("Complete RF analysis for more downloads")
                    
        else:
            st.error("Analysis data is incomplete. Please run the analysis again.")

    else:
        # Landing page when no file is uploaded
        st.markdown("""
        ### ðŸš€ Getting Started
        
        1. **Upload your data** using the file uploader in the sidebar
        2. **Select your target column** (the variable you want to predict)
        3. **Optional: Filter by reference** if your data contains multiple experimental batches
        4. **Optional: Exclude specific columns** manually before analysis
        5. **Click "Run Analysis"** to start the automated analysis
        
        ### ðŸ“Š What this tool does:
        
        - **Reference Filtering**: Analyze specific experimental batches or references
        - **Manual Column Exclusion**: Choose specific columns to exclude from analysis
        - **Automated Data Preprocessing**: Handles missing values, removes sparse columns
        - **Univariate Regression**: Tests each feature individually against target
        - **Statistical Significance**: Filters features based on p-value thresholds
        - **Random Forest Modeling**: Advanced ML with grid search optimization
        - **Small Dataset Optimization**: Option to skip train/test split for small datasets
        - **Data Visualizations**: Correlation heatmaps, predicted vs actual plots
        - **Downloadable Results**: Get CSV files of all analysis results
        
        ### ðŸ“ Supported file formats:
        - CSV files (`.csv`)
        - Excel files (`.xlsx`, `.xls`)
        
        ### ðŸ”¬ Reference Filtering:
        - Select a column containing experimental references, batch IDs, or study names
        - Choose which specific reference to analyze
        - Ensures analysis only uses data from the same experimental condition
        
        ### ðŸš« Manual Column Exclusion:
        - Select specific columns to exclude from analysis before preprocessing
        - Useful for removing known irrelevant features, ID columns, or text fields
        - Excluded columns will be marked as "Manually Excluded" in preprocessing results
        - Works in addition to automatic preprocessing exclusions
        
        ### ðŸŒ² Random Forest Options:
        - **Small Dataset Mode**: Skip train/test split to use all data for training
        - **Grid Search**: Automatically optimize hyperparameters (n_estimators, max_depth, etc.)
        - **Cross-Validation**: Always uses 5-fold CV for robust performance estimates
        - **Feature Importance**: Default RF importance, permutation importance, or custom grouped importance
        """)

if __name__ == "__main__":
    main()

